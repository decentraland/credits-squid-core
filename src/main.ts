// main.js
// This is the main executable of the squid indexer.

// EvmBatchProcessor is the class responsible for data retrieval and processing.
import { assertNotNull, EvmBatchProcessor } from "@subsquid/evm-processor";
// TypeormDatabase is the class responsible for data storage.
import { TypeormDatabase } from "@subsquid/typeorm-store";
// usdcAbi is a utility module generated from the JSON ABI of the USDC contract.
// It contains methods for event decoding, direct RPC queries and some useful
// constants.
import * as creditAbi from "./abi/credits";
// UsdcTransfer is a TypeORM model class of the usdc_transfer Postgres table.
// It is autogenerated from schema.graphql.
import {
  // CreditGrant,
  CreditConsumption,
  // CreditValidation,
  // CreditTransfer,
} from "./model";
import { Store } from "@subsquid/typeorm-store";
import {
  UserCreditStats,
  HourlyCreditUsage,
  DailyCreditUsage,
  MarketplaceCreditUsage,
} from "./model";
import { events } from "./abi/credits";

// Amoy testnet contract address
const CREDITS_CONTRACT_ADDRESS = "0xb3f1d3e806cf2ec822ad32c01ad64a1995b67752";
const RPC_ENDPOINT = process.env.RPC_ENDPOINT_POLYGON;

// First we configure data retrieval.
const processor = new EvmBatchProcessor()
  // .setDataSource({
  //   archive: 'https://v2.archive.subsquid.io/network/amoy-testnet',
  //   chain: 'https://rpc.amoy.dev',
  // })
  .setGateway("https://v2.archive.subsquid.io/network/polygon-amoy-testnet")
  .setRpcEndpoint({
    url: assertNotNull(RPC_ENDPOINT),
    rateLimit: 10,
  })
  .setFinalityConfirmation(75)
  .setBlockRange({ from: 17942200 })
  .setFields({
    log: {
      transactionHash: true,
    },
  })
  .addLog({
    address: [CREDITS_CONTRACT_ADDRESS],
    topic0: [events.CreditSpent.topic],
  });

// TypeormDatabase objects store the data to Postgres. They are capable of
// handling the rollbacks that occur due to blockchain forks.
//
// There are also Database classes for storing data to files and BigQuery
// datasets.
const db = new TypeormDatabase();

// The processor.run() call executes the data processing. Its second argument is
// the handler function that is executed once on each batch of data. Processor
// object provides the data via "ctx.blocks". However, the handler can contain
// arbitrary TypeScript code, so it's OK to bring in extra data from IPFS,
// direct RPC calls, external APIs etc.
processor.run(db, async (ctx) => {
  console.log("Processing new batch of blocks...");
  console.log(
    `Batch range: ${ctx.blocks[0]?.header.height} -> ${
      ctx.blocks[ctx.blocks.length - 1]?.header.height
    }`
  );

  const consumptions: CreditConsumption[] = [];
  const userStats = new Map<string, UserCreditStats>();
  const hourlyUsage = new Map<string, HourlyCreditUsage>();
  const dailyUsage = new Map<string, DailyCreditUsage>();

  for (let block of ctx.blocks) {
    for (let log of block.logs) {
      if (
        // log.address === CREDITS_CONTRACT_ADDRESS &&
        log.topics[0] === events.CreditSpent.topic
      ) {
        console.log("ðŸŽ¯ Found CreditSpent event!");

        const { _creditId, beneficiary, amount } =
          events.CreditSpent.decode(log);
        console.log({
          event: "CreditSpent",
          creditId: _creditId,
          beneficiary,
          amount: amount.toString(),
          blockNumber: block.header.height,
          txHash: log.transactionHash,
        });

        const timestamp = new Date(block.header.timestamp);

        // Get or create UserCreditStats
        let userStat = userStats.get(beneficiary);
        if (!userStat) {
          const existingStats = await ctx.store.get(
            UserCreditStats,
            beneficiary
          );
          console.log(
            existingStats
              ? `Found existing stats for user ${beneficiary}`
              : `Creating new stats for user ${beneficiary}`
          );

          userStat =
            existingStats ||
            new UserCreditStats({
              id: beneficiary,
              address: beneficiary,
              totalCreditsConsumed: 0n,
            });
          userStats.set(beneficiary, userStat);
        }

        const oldTotal = userStat.totalCreditsConsumed;
        userStat.totalCreditsConsumed += amount;
        userStat.lastCreditUsage = timestamp;

        console.log(`Updated user stats:`, {
          user: beneficiary,
          oldTotal: oldTotal.toString(),
          newTotal: userStat.totalCreditsConsumed.toString(),
          lastUsage: userStat.lastCreditUsage,
        });

        // Create credit consumption record
        const consumption = new CreditConsumption({
          id: log.id,
          creditId: _creditId,
          beneficiary: userStat,
          amount,
          timestamp,
          block: block.header.height,
          txHash: log.transactionHash,
        });
        consumptions.push(consumption);
        console.log("Created consumption record:", {
          id: consumption.id,
          amount: consumption.amount.toString(),
          block: consumption.block,
        });

        // Update hourly usage
        const hourKey = `${timestamp.getUTCFullYear()}-${timestamp.getUTCMonth()}-${timestamp.getUTCDate()}-${timestamp.getUTCHours()}`;
        let hourUsage = hourlyUsage.get(hourKey);
        if (!hourUsage) {
          hourUsage =
            (await ctx.store.get(HourlyCreditUsage, hourKey)) ||
            new HourlyCreditUsage({
              id: hourKey,
              totalAmount: 0n,
              usageCount: 0,
              timestamp,
            });
        }
        hourUsage.totalAmount += amount;
        hourUsage.usageCount += 1;
        hourlyUsage.set(hourKey, hourUsage);
        console.log("Updated hourly stats:", {
          hour: hourKey,
          totalAmount: hourUsage.totalAmount.toString(),
          usageCount: hourUsage.usageCount,
        });

        // Update daily usage
        const dayKey = `${timestamp.getUTCFullYear()}-${timestamp.getUTCMonth()}-${timestamp.getUTCDate()}`;
        let dayUsage = dailyUsage.get(dayKey);
        if (!dayUsage) {
          dayUsage =
            (await ctx.store.get(DailyCreditUsage, dayKey)) ||
            new DailyCreditUsage({
              id: dayKey,
              totalAmount: 0n,
              uniqueUsers: 0,
              usageCount: 0,
              timestamp,
            });
        }
        dayUsage.totalAmount += amount;
        dayUsage.usageCount += 1;
        dailyUsage.set(dayKey, dayUsage);
        console.log("Updated daily stats:", {
          day: dayKey,
          totalAmount: dayUsage.totalAmount.toString(),
          usageCount: dayUsage.usageCount,
        });
      }
    }
  }

  // Only log unique users if we have any daily usage
  if (dailyUsage.size > 0) {
    console.log("\nUpdating daily unique users counts...");
    for (let [dayKey, usage] of dailyUsage) {
      const uniqueUsers = new Set(
        consumptions
          .filter((c) => {
            const d = c.timestamp;
            return (
              `${d.getUTCFullYear()}-${d.getUTCMonth()}-${d.getUTCDate()}` ===
              dayKey
            );
          })
          .map((c) => c.beneficiary.id)
      );
      usage.uniqueUsers = uniqueUsers.size;
      console.log(`Day ${dayKey}: ${usage.uniqueUsers} unique users`);
    }
  }

  // Only log and save if we have entities to save
  const hasEntities =
    userStats.size > 0 ||
    hourlyUsage.size > 0 ||
    dailyUsage.size > 0 ||
    consumptions.length > 0;

  if (hasEntities) {
    console.log("\nSaving entities to database...");
    console.log(`- ${userStats.size} user stats`);
    console.log(`- ${hourlyUsage.size} hourly records`);
    console.log(`- ${dailyUsage.size} daily records`);
    console.log(`- ${consumptions.length} consumption records`);

    await ctx.store.save([...userStats.values()]);
    await ctx.store.save([...hourlyUsage.values()]);
    await ctx.store.save([...dailyUsage.values()]);
    await ctx.store.save(consumptions);

    console.log("Batch processing complete! âœ¨\n");
  }
});
